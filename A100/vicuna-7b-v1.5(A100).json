{
  "results": {
    "hendrycksTest-virology": {
      "acc": 0.41566265060240964,
      "acc_stderr": 0.03836722176598053,
      "acc_norm": 0.41566265060240964,
      "acc_norm_stderr": 0.03836722176598053
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542126,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542126
    },
    "hellaswag": {
      "acc": 0.5649273053176658,
      "acc_stderr": 0.004947533158712099,
      "acc_norm": 0.73770165305716,
      "acc_norm_stderr": 0.0043898499070403095
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.5277777777777778,
      "acc_stderr": 0.048262172941398944,
      "acc_norm": 0.5277777777777778,
      "acc_norm_stderr": 0.048262172941398944
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.6896551724137931,
      "acc_stderr": 0.016543785026048304,
      "acc_norm": 0.6896551724137931,
      "acc_norm_stderr": 0.016543785026048304
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.45,
      "acc_stderr": 0.05,
      "acc_norm": 0.45,
      "acc_norm_stderr": 0.05
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.49264705882352944,
      "acc_stderr": 0.030369552523902173,
      "acc_norm": 0.49264705882352944,
      "acc_norm_stderr": 0.030369552523902173
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.4827586206896552,
      "acc_stderr": 0.04164188720169377,
      "acc_norm": 0.4827586206896552,
      "acc_norm_stderr": 0.04164188720169377
    },
    "hendrycksTest-human_aging": {
      "acc": 0.5829596412556054,
      "acc_stderr": 0.03309266936071721,
      "acc_norm": 0.5829596412556054,
      "acc_norm_stderr": 0.03309266936071721
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.71,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.71,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.6484848484848484,
      "acc_stderr": 0.037282069986826503,
      "acc_norm": 0.6484848484848484,
      "acc_norm_stderr": 0.037282069986826503
    },
    "hendrycksTest-marketing": {
      "acc": 0.7863247863247863,
      "acc_stderr": 0.026853450377009154,
      "acc_norm": 0.7863247863247863,
      "acc_norm_stderr": 0.026853450377009154
    },
    "hendrycksTest-prehistory": {
      "acc": 0.5092592592592593,
      "acc_stderr": 0.027815973433878014,
      "acc_norm": 0.5092592592592593,
      "acc_norm_stderr": 0.027815973433878014
    },
    "hendrycksTest-world_religions": {
      "acc": 0.7192982456140351,
      "acc_stderr": 0.034462962170884265,
      "acc_norm": 0.7192982456140351,
      "acc_norm_stderr": 0.034462962170884265
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5636363636363636,
      "acc_stderr": 0.04750185058907297,
      "acc_norm": 0.5636363636363636,
      "acc_norm_stderr": 0.04750185058907297
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.567741935483871,
      "acc_stderr": 0.028181739720019416,
      "acc_norm": 0.567741935483871,
      "acc_norm_stderr": 0.028181739720019416
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.6911764705882353,
      "acc_stderr": 0.032426617198272174,
      "acc_norm": 0.6911764705882353,
      "acc_norm_stderr": 0.032426617198272174
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.6735751295336787,
      "acc_stderr": 0.033840286211432945,
      "acc_norm": 0.6735751295336787,
      "acc_norm_stderr": 0.033840286211432945
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.040061680838488774,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.040061680838488774
    },
    "hendrycksTest-college_physics": {
      "acc": 0.20588235294117646,
      "acc_stderr": 0.04023382273617747,
      "acc_norm": 0.20588235294117646,
      "acc_norm_stderr": 0.04023382273617747
    },
    "hendrycksTest-abstract_algebra": {
      "acc": 0.32,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-philosophy": {
      "acc": 0.572347266881029,
      "acc_stderr": 0.02809924077580956,
      "acc_norm": 0.572347266881029,
      "acc_norm_stderr": 0.02809924077580956
    },
    "hendrycksTest-econometrics": {
      "acc": 0.22807017543859648,
      "acc_stderr": 0.03947152782669415,
      "acc_norm": 0.22807017543859648,
      "acc_norm_stderr": 0.03947152782669415
    },
    "hendrycksTest-computer_security": {
      "acc": 0.62,
      "acc_stderr": 0.048783173121456316,
      "acc_norm": 0.62,
      "acc_norm_stderr": 0.048783173121456316
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.46923076923076923,
      "acc_stderr": 0.025302958890850154,
      "acc_norm": 0.46923076923076923,
      "acc_norm_stderr": 0.025302958890850154
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6666666666666666,
      "acc_stderr": 0.0306858205966108,
      "acc_norm": 0.6666666666666666,
      "acc_norm_stderr": 0.0306858205966108
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.42,
      "acc_stderr": 0.04960449637488584,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.04960449637488584
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.5471698113207547,
      "acc_stderr": 0.03063562795796182,
      "acc_norm": 0.5471698113207547,
      "acc_norm_stderr": 0.03063562795796182
    },
    "hendrycksTest-anatomy": {
      "acc": 0.45185185185185184,
      "acc_stderr": 0.04299268905480864,
      "acc_norm": 0.45185185185185184,
      "acc_norm_stderr": 0.04299268905480864
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2980132450331126,
      "acc_stderr": 0.03734535676787198,
      "acc_norm": 0.2980132450331126,
      "acc_norm_stderr": 0.03734535676787198
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.4107142857142857,
      "acc_stderr": 0.04669510663875191,
      "acc_norm": 0.4107142857142857,
      "acc_norm_stderr": 0.04669510663875191
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.53,
      "acc_stderr": 0.05016135580465919,
      "acc_norm": 0.53,
      "acc_norm_stderr": 0.05016135580465919
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.5858585858585859,
      "acc_stderr": 0.03509438348879629,
      "acc_norm": 0.5858585858585859,
      "acc_norm_stderr": 0.03509438348879629
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.3971631205673759,
      "acc_stderr": 0.0291898056735871,
      "acc_norm": 0.3971631205673759,
      "acc_norm_stderr": 0.0291898056735871
    },
    "hendrycksTest-nutrition": {
      "acc": 0.5032679738562091,
      "acc_stderr": 0.02862930519400354,
      "acc_norm": 0.5032679738562091,
      "acc_norm_stderr": 0.02862930519400354
    },
    "truthfulqa_mc": {
      "mc1": 0.3329253365973072,
      "mc1_stderr": 0.016497402382012052,
      "mc2": 0.5039710774406772,
      "mc2_stderr": 0.015633180204076506
    },
    "hendrycksTest-sociology": {
      "acc": 0.7263681592039801,
      "acc_stderr": 0.03152439186555402,
      "acc_norm": 0.7263681592039801,
      "acc_norm_stderr": 0.03152439186555402
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2446927374301676,
      "acc_stderr": 0.014378169884098416,
      "acc_norm": 0.2446927374301676,
      "acc_norm_stderr": 0.014378169884098416
    },
    "hendrycksTest-management": {
      "acc": 0.6407766990291263,
      "acc_stderr": 0.04750458399041696,
      "acc_norm": 0.6407766990291263,
      "acc_norm_stderr": 0.04750458399041696
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621504,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621504
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.5433526011560693,
      "acc_stderr": 0.026817718130348923,
      "acc_norm": 0.5433526011560693,
      "acc_norm_stderr": 0.026817718130348923
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.6106870229007634,
      "acc_stderr": 0.04276486542814591,
      "acc_norm": 0.6106870229007634,
      "acc_norm_stderr": 0.04276486542814591
    },
    "hendrycksTest-college_biology": {
      "acc": 0.5833333333333334,
      "acc_stderr": 0.04122728707651282,
      "acc_norm": 0.5833333333333334,
      "acc_norm_stderr": 0.04122728707651282
    },
    "hendrycksTest-international_law": {
      "acc": 0.628099173553719,
      "acc_stderr": 0.044120158066245044,
      "acc_norm": 0.628099173553719,
      "acc_norm_stderr": 0.044120158066245044
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.30952380952380953,
      "acc_stderr": 0.023809523809523867,
      "acc_norm": 0.30952380952380953,
      "acc_norm_stderr": 0.023809523809523867
    },
    "hendrycksTest-security_studies": {
      "acc": 0.6081632653061224,
      "acc_stderr": 0.03125127591089165,
      "acc_norm": 0.6081632653061224,
      "acc_norm_stderr": 0.03125127591089165
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3194444444444444,
      "acc_stderr": 0.03179876342176852,
      "acc_norm": 0.3194444444444444,
      "acc_norm_stderr": 0.03179876342176852
    },
    "arc_challenge": {
      "acc": 0.4283276450511945,
      "acc_stderr": 0.014460496367599024,
      "acc_norm": 0.46075085324232085,
      "acc_norm_stderr": 0.014566303676636588
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.4508670520231214,
      "acc_stderr": 0.03794012674697029,
      "acc_norm": 0.4508670520231214,
      "acc_norm_stderr": 0.03794012674697029
    },
    "hendrycksTest-astronomy": {
      "acc": 0.4342105263157895,
      "acc_stderr": 0.04033565667848319,
      "acc_norm": 0.4342105263157895,
      "acc_norm_stderr": 0.04033565667848319
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.3793103448275862,
      "acc_stderr": 0.03413963805906235,
      "acc_norm": 0.3793103448275862,
      "acc_norm_stderr": 0.03413963805906235
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.25555555555555554,
      "acc_stderr": 0.026593939101844086,
      "acc_norm": 0.25555555555555554,
      "acc_norm_stderr": 0.026593939101844086
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.4722222222222222,
      "acc_stderr": 0.0201965949335412,
      "acc_norm": 0.4722222222222222,
      "acc_norm_stderr": 0.0201965949335412
    },
    "hendrycksTest-professional_law": {
      "acc": 0.38461538461538464,
      "acc_stderr": 0.012425548416302942,
      "acc_norm": 0.38461538461538464,
      "acc_norm_stderr": 0.012425548416302942
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.4369747899159664,
      "acc_stderr": 0.03221943636566196,
      "acc_norm": 0.4369747899159664,
      "acc_norm_stderr": 0.03221943636566196
    },
    "hendrycksTest-global_facts": {
      "acc": 0.36,
      "acc_stderr": 0.048241815132442176,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.048241815132442176
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.671559633027523,
      "acc_stderr": 0.020135902797298412,
      "acc_norm": 0.671559633027523,
      "acc_norm_stderr": 0.020135902797298412
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.55,
      "acc_stderr": 0.05,
      "acc_norm": 0.55,
      "acc_norm_stderr": 0.05
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.39148936170212767,
      "acc_stderr": 0.031907012423268113,
      "acc_norm": 0.39148936170212767,
      "acc_norm_stderr": 0.031907012423268113
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.588957055214724,
      "acc_stderr": 0.038656978537853624,
      "acc_norm": 0.588957055214724,
      "acc_norm_stderr": 0.038656978537853624
    }
  },
  "versions": {
    "hendrycksTest-virology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hellaswag": 0,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-world_religions": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-nutrition": 1,
    "truthfulqa_mc": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-high_school_statistics": 1,
    "arc_challenge": 0,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-logical_fallacies": 1
  },
  "config": {
    "model": "vicuna-7b-v1.5",
    "num_fewshot": 0,
    "batch_size": 4,
    "device": "cuda:0",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 2,
    "description_dict": null
  }
}