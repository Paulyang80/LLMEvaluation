{
  "results": {
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.689119170984456,
      "acc_stderr": 0.03340361906276585,
      "acc_norm": 0.689119170984456,
      "acc_norm_stderr": 0.03340361906276585
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.6896551724137931,
      "acc_stderr": 0.01654378502604831,
      "acc_norm": 0.6896551724137931,
      "acc_norm_stderr": 0.01654378502604831
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.36607142857142855,
      "acc_stderr": 0.0457237235873743,
      "acc_norm": 0.36607142857142855,
      "acc_norm_stderr": 0.0457237235873743
    },
    "truthfulqa_mc": {
      "mc1": 0.3023255813953488,
      "mc1_stderr": 0.01607750926613303,
      "mc2": 0.4539376590278185,
      "mc2_stderr": 0.015639799214843934
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.36524822695035464,
      "acc_stderr": 0.028723863853281285,
      "acc_norm": 0.36524822695035464,
      "acc_norm_stderr": 0.028723863853281285
    },
    "hendrycksTest-international_law": {
      "acc": 0.5950413223140496,
      "acc_stderr": 0.04481137755942469,
      "acc_norm": 0.5950413223140496,
      "acc_norm_stderr": 0.04481137755942469
    },
    "hendrycksTest-college_physics": {
      "acc": 0.19607843137254902,
      "acc_stderr": 0.03950581861179962,
      "acc_norm": 0.19607843137254902,
      "acc_norm_stderr": 0.03950581861179962
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.5509433962264151,
      "acc_stderr": 0.030612730713641095,
      "acc_norm": 0.5509433962264151,
      "acc_norm_stderr": 0.030612730713641095
    },
    "arc_challenge": {
      "acc": 0.44112627986348124,
      "acc_stderr": 0.014509747749064663,
      "acc_norm": 0.44368600682593856,
      "acc_norm_stderr": 0.01451842182567044
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.5644171779141104,
      "acc_stderr": 0.03895632464138938,
      "acc_norm": 0.5644171779141104,
      "acc_norm_stderr": 0.03895632464138938
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.32,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-virology": {
      "acc": 0.4759036144578313,
      "acc_stderr": 0.038879718495972646,
      "acc_norm": 0.4759036144578313,
      "acc_norm_stderr": 0.038879718495972646
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.4482758620689655,
      "acc_stderr": 0.04144311810878152,
      "acc_norm": 0.4482758620689655,
      "acc_norm_stderr": 0.04144311810878152
    },
    "hellaswag": {
      "acc": 0.5781716789484167,
      "acc_stderr": 0.004928420903026553,
      "acc_norm": 0.7542322246564429,
      "acc_norm_stderr": 0.004296615862786625
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.5818181818181818,
      "acc_stderr": 0.03851716319398394,
      "acc_norm": 0.5818181818181818,
      "acc_norm_stderr": 0.03851716319398394
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.47,
      "acc_stderr": 0.050161355804659205,
      "acc_norm": 0.47,
      "acc_norm_stderr": 0.050161355804659205
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.33004926108374383,
      "acc_stderr": 0.033085304262282574,
      "acc_norm": 0.33004926108374383,
      "acc_norm_stderr": 0.033085304262282574
    },
    "hendrycksTest-philosophy": {
      "acc": 0.5305466237942122,
      "acc_stderr": 0.028345045864840608,
      "acc_norm": 0.5305466237942122,
      "acc_norm_stderr": 0.028345045864840608
    },
    "hendrycksTest-professional_law": {
      "acc": 0.35984354628422427,
      "acc_stderr": 0.0122582604836898,
      "acc_norm": 0.35984354628422427,
      "acc_norm_stderr": 0.0122582604836898
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.4935483870967742,
      "acc_stderr": 0.02844163823354051,
      "acc_norm": 0.4935483870967742,
      "acc_norm_stderr": 0.02844163823354051
    },
    "hendrycksTest-marketing": {
      "acc": 0.7521367521367521,
      "acc_stderr": 0.028286324075564386,
      "acc_norm": 0.7521367521367521,
      "acc_norm_stderr": 0.028286324075564386
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2740740740740741,
      "acc_stderr": 0.027195934804085622,
      "acc_norm": 0.2740740740740741,
      "acc_norm_stderr": 0.027195934804085622
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2424581005586592,
      "acc_stderr": 0.014333522059217889,
      "acc_norm": 0.2424581005586592,
      "acc_norm_stderr": 0.014333522059217889
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2847682119205298,
      "acc_stderr": 0.03684881521389023,
      "acc_norm": 0.2847682119205298,
      "acc_norm_stderr": 0.03684881521389023
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.5648854961832062,
      "acc_stderr": 0.04348208051644858,
      "acc_norm": 0.5648854961832062,
      "acc_norm_stderr": 0.04348208051644858
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.023266512213730557,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.023266512213730557
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.040061680838488774,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.040061680838488774
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.4,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.6238532110091743,
      "acc_stderr": 0.02076923196820508,
      "acc_norm": 0.6238532110091743,
      "acc_norm_stderr": 0.02076923196820508
    },
    "hendrycksTest-college_biology": {
      "acc": 0.4583333333333333,
      "acc_stderr": 0.04166666666666666,
      "acc_norm": 0.4583333333333333,
      "acc_norm_stderr": 0.04166666666666666
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.4,
      "acc_stderr": 0.03202563076101736,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.03202563076101736
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.46568627450980393,
      "acc_stderr": 0.020180144843307293,
      "acc_norm": 0.46568627450980393,
      "acc_norm_stderr": 0.020180144843307293
    },
    "hendrycksTest-nutrition": {
      "acc": 0.48366013071895425,
      "acc_stderr": 0.028614624752805407,
      "acc_norm": 0.48366013071895425,
      "acc_norm_stderr": 0.028614624752805407
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.031124619309328177,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.031124619309328177
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.71,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.71,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-astronomy": {
      "acc": 0.48026315789473684,
      "acc_stderr": 0.04065771002562605,
      "acc_norm": 0.48026315789473684,
      "acc_norm_stderr": 0.04065771002562605
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.3930635838150289,
      "acc_stderr": 0.0372424959581773,
      "acc_norm": 0.3930635838150289,
      "acc_norm_stderr": 0.0372424959581773
    },
    "hendrycksTest-computer_security": {
      "acc": 0.61,
      "acc_stderr": 0.04902071300001975,
      "acc_norm": 0.61,
      "acc_norm_stderr": 0.04902071300001975
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.2638888888888889,
      "acc_stderr": 0.03005820270430985,
      "acc_norm": 0.2638888888888889,
      "acc_norm_stderr": 0.03005820270430985
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.4117647058823529,
      "acc_stderr": 0.029896163033125464,
      "acc_norm": 0.4117647058823529,
      "acc_norm_stderr": 0.029896163033125464
    },
    "hendrycksTest-prehistory": {
      "acc": 0.5432098765432098,
      "acc_stderr": 0.027716661650194038,
      "acc_norm": 0.5432098765432098,
      "acc_norm_stderr": 0.027716661650194038
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5181818181818182,
      "acc_stderr": 0.04785964010794916,
      "acc_norm": 0.5181818181818182,
      "acc_norm_stderr": 0.04785964010794916
    },
    "hendrycksTest-anatomy": {
      "acc": 0.45185185185185184,
      "acc_stderr": 0.04299268905480864,
      "acc_norm": 0.45185185185185184,
      "acc_norm_stderr": 0.04299268905480864
    },
    "hendrycksTest-management": {
      "acc": 0.6796116504854369,
      "acc_stderr": 0.04620284082280042,
      "acc_norm": 0.6796116504854369,
      "acc_norm_stderr": 0.04620284082280042
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.6617647058823529,
      "acc_stderr": 0.03320574612945431,
      "acc_norm": 0.6617647058823529,
      "acc_norm_stderr": 0.03320574612945431
    },
    "hendrycksTest-abstract_algebra": {
      "acc": 0.3,
      "acc_stderr": 0.04605661864718381,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.04605661864718381
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621505,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621505
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6160337552742616,
      "acc_stderr": 0.031658678064106674,
      "acc_norm": 0.6160337552742616,
      "acc_norm_stderr": 0.031658678064106674
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.5909090909090909,
      "acc_stderr": 0.03502975799413007,
      "acc_norm": 0.5909090909090909,
      "acc_norm_stderr": 0.03502975799413007
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.5,
      "acc_stderr": 0.026919095102908273,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.026919095102908273
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.46,
      "acc_stderr": 0.05009082659620332,
      "acc_norm": 0.46,
      "acc_norm_stderr": 0.05009082659620332
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2894736842105263,
      "acc_stderr": 0.04266339443159393,
      "acc_norm": 0.2894736842105263,
      "acc_norm_stderr": 0.04266339443159393
    },
    "hendrycksTest-global_facts": {
      "acc": 0.4,
      "acc_stderr": 0.04923659639173309,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.04923659639173309
    },
    "hendrycksTest-sociology": {
      "acc": 0.736318407960199,
      "acc_stderr": 0.03115715086935556,
      "acc_norm": 0.736318407960199,
      "acc_norm_stderr": 0.03115715086935556
    },
    "hendrycksTest-world_religions": {
      "acc": 0.6900584795321637,
      "acc_stderr": 0.035469769593931624,
      "acc_norm": 0.6900584795321637,
      "acc_norm_stderr": 0.035469769593931624
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.40512820512820513,
      "acc_stderr": 0.024890471769938145,
      "acc_norm": 0.40512820512820513,
      "acc_norm_stderr": 0.024890471769938145
    },
    "hendrycksTest-security_studies": {
      "acc": 0.5142857142857142,
      "acc_stderr": 0.03199615232806287,
      "acc_norm": 0.5142857142857142,
      "acc_norm_stderr": 0.03199615232806287
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.5555555555555556,
      "acc_stderr": 0.04803752235190193,
      "acc_norm": 0.5555555555555556,
      "acc_norm_stderr": 0.04803752235190193
    },
    "hendrycksTest-human_aging": {
      "acc": 0.5919282511210763,
      "acc_stderr": 0.03298574607842822,
      "acc_norm": 0.5919282511210763,
      "acc_norm_stderr": 0.03298574607842822
    }
  },
  "versions": {
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-machine_learning": 1,
    "truthfulqa_mc": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "arc_challenge": 0,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hellaswag": 0,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-world_religions": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-human_aging": 1
  },
  "config": {
    "model": "Llama-2-7b-chat-hf",
    "num_fewshot": 0,
    "batch_size": 4,
    "device": "cuda:0",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 2,
    "description_dict": null
  }
}