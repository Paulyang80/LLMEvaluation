{
  "results": {
    "hendrycksTest-anatomy": {
      "acc": 0.21481481481481482,
      "acc_stderr": 0.03547854198560825,
      "acc_norm": 0.21481481481481482,
      "acc_norm_stderr": 0.03547854198560825
    },
    "hendrycksTest-global_facts": {
      "acc": 0.26,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.0440844002276808
    },
    "hendrycksTest-philosophy": {
      "acc": 0.2347266881028939,
      "acc_stderr": 0.024071805887677048,
      "acc_norm": 0.2347266881028939,
      "acc_norm_stderr": 0.024071805887677048
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.3392857142857143,
      "acc_stderr": 0.044939490686135404,
      "acc_norm": 0.3392857142857143,
      "acc_norm_stderr": 0.044939490686135404
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.28078817733990147,
      "acc_stderr": 0.0316185633535861,
      "acc_norm": 0.28078817733990147,
      "acc_norm_stderr": 0.0316185633535861
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.26717557251908397,
      "acc_stderr": 0.03880848301082396,
      "acc_norm": 0.26717557251908397,
      "acc_norm_stderr": 0.03880848301082396
    },
    "hendrycksTest-international_law": {
      "acc": 0.32231404958677684,
      "acc_stderr": 0.04266416363352167,
      "acc_norm": 0.32231404958677684,
      "acc_norm_stderr": 0.04266416363352167
    },
    "hellaswag": {
      "acc": 0.3309101772555268,
      "acc_stderr": 0.004695791340502862,
      "acc_norm": 0.38408683529177456,
      "acc_norm_stderr": 0.004853845750392154
    },
    "hendrycksTest-security_studies": {
      "acc": 0.2612244897959184,
      "acc_stderr": 0.028123429335142783,
      "acc_norm": 0.2612244897959184,
      "acc_norm_stderr": 0.028123429335142783
    },
    "hendrycksTest-college_biology": {
      "acc": 0.2569444444444444,
      "acc_stderr": 0.03653946969442099,
      "acc_norm": 0.2569444444444444,
      "acc_norm_stderr": 0.03653946969442099
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.21818181818181817,
      "acc_stderr": 0.03225078108306289,
      "acc_norm": 0.21818181818181817,
      "acc_norm_stderr": 0.03225078108306289
    },
    "truthfulqa_mc": {
      "mc1": 0.23133414932680538,
      "mc1_stderr": 0.014761945174862677,
      "mc2": 0.4063592919200308,
      "mc2_stderr": 0.014587176356549771
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.26490066225165565,
      "acc_stderr": 0.03603038545360384,
      "acc_norm": 0.26490066225165565,
      "acc_norm_stderr": 0.03603038545360384
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.25980392156862747,
      "acc_stderr": 0.030778554678693264,
      "acc_norm": 0.25980392156862747,
      "acc_norm_stderr": 0.030778554678693264
    },
    "hendrycksTest-human_aging": {
      "acc": 0.2645739910313901,
      "acc_stderr": 0.029605103217038332,
      "acc_norm": 0.2645739910313901,
      "acc_norm_stderr": 0.029605103217038332
    },
    "hendrycksTest-college_physics": {
      "acc": 0.17647058823529413,
      "acc_stderr": 0.0379328118530781,
      "acc_norm": 0.17647058823529413,
      "acc_norm_stderr": 0.0379328118530781
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.18253968253968253,
      "acc_stderr": 0.03455071019102147,
      "acc_norm": 0.18253968253968253,
      "acc_norm_stderr": 0.03455071019102147
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.33,
      "acc_stderr": 0.04725815626252604,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252604
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.22258064516129034,
      "acc_stderr": 0.023664216671642535,
      "acc_norm": 0.22258064516129034,
      "acc_norm_stderr": 0.023664216671642535
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.2543352601156069,
      "acc_stderr": 0.023445826276545543,
      "acc_norm": 0.2543352601156069,
      "acc_norm_stderr": 0.023445826276545543
    },
    "hendrycksTest-sociology": {
      "acc": 0.23383084577114427,
      "acc_stderr": 0.029929415408348373,
      "acc_norm": 0.23383084577114427,
      "acc_norm_stderr": 0.029929415408348373
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.21,
      "acc_stderr": 0.04093601807403326,
      "acc_norm": 0.21,
      "acc_norm_stderr": 0.04093601807403326
    },
    "hendrycksTest-public_relations": {
      "acc": 0.23636363636363636,
      "acc_stderr": 0.04069306319721377,
      "acc_norm": 0.23636363636363636,
      "acc_norm_stderr": 0.04069306319721377
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.26,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.0440844002276808
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.2037037037037037,
      "acc_stderr": 0.02746740180405799,
      "acc_norm": 0.2037037037037037,
      "acc_norm_stderr": 0.02746740180405799
    },
    "hendrycksTest-prehistory": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.02378858355165854,
      "acc_norm": 0.24074074074074073,
      "acc_norm_stderr": 0.02378858355165854
    },
    "hendrycksTest-computer_security": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695235
    },
    "hendrycksTest-professional_law": {
      "acc": 0.25684485006518903,
      "acc_stderr": 0.011158455853098857,
      "acc_norm": 0.25684485006518903,
      "acc_norm_stderr": 0.011158455853098857
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.21693121693121692,
      "acc_stderr": 0.02122708244944507,
      "acc_norm": 0.21693121693121692,
      "acc_norm_stderr": 0.02122708244944507
    },
    "hendrycksTest-marketing": {
      "acc": 0.32051282051282054,
      "acc_stderr": 0.030572811310299607,
      "acc_norm": 0.32051282051282054,
      "acc_norm_stderr": 0.030572811310299607
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.2188679245283019,
      "acc_stderr": 0.025447863825108618,
      "acc_norm": 0.2188679245283019,
      "acc_norm_stderr": 0.025447863825108618
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.15808823529411764,
      "acc_stderr": 0.02216146260806851,
      "acc_norm": 0.15808823529411764,
      "acc_norm_stderr": 0.02216146260806851
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.2482758620689655,
      "acc_stderr": 0.036001056927277716,
      "acc_norm": 0.2482758620689655,
      "acc_norm_stderr": 0.036001056927277716
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.23302752293577983,
      "acc_stderr": 0.018125669180861507,
      "acc_norm": 0.23302752293577983,
      "acc_norm_stderr": 0.018125669180861507
    },
    "hendrycksTest-virology": {
      "acc": 0.2469879518072289,
      "acc_stderr": 0.03357351982064536,
      "acc_norm": 0.2469879518072289,
      "acc_norm_stderr": 0.03357351982064536
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.20809248554913296,
      "acc_stderr": 0.030952890217749884,
      "acc_norm": 0.20809248554913296,
      "acc_norm_stderr": 0.030952890217749884
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.043300437496507416,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.043300437496507416
    },
    "hendrycksTest-econometrics": {
      "acc": 0.17543859649122806,
      "acc_stderr": 0.03577954813948368,
      "acc_norm": 0.17543859649122806,
      "acc_norm_stderr": 0.03577954813948368
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.2826797385620915,
      "acc_stderr": 0.01821726955205343,
      "acc_norm": 0.2826797385620915,
      "acc_norm_stderr": 0.01821726955205343
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.2076923076923077,
      "acc_stderr": 0.020567539567246804,
      "acc_norm": 0.2076923076923077,
      "acc_norm_stderr": 0.020567539567246804
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.25957446808510637,
      "acc_stderr": 0.028659179374292337,
      "acc_norm": 0.25957446808510637,
      "acc_norm_stderr": 0.028659179374292337
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.18,
      "acc_stderr": 0.038612291966536955,
      "acc_norm": 0.18,
      "acc_norm_stderr": 0.038612291966536955
    },
    "hendrycksTest-nutrition": {
      "acc": 0.25163398692810457,
      "acc_stderr": 0.0248480182638752,
      "acc_norm": 0.25163398692810457,
      "acc_norm_stderr": 0.0248480182638752
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.21481481481481482,
      "acc_stderr": 0.025040443877000686,
      "acc_norm": 0.21481481481481482,
      "acc_norm_stderr": 0.025040443877000686
    },
    "hendrycksTest-world_religions": {
      "acc": 0.26900584795321636,
      "acc_stderr": 0.0340105262010409,
      "acc_norm": 0.26900584795321636,
      "acc_norm_stderr": 0.0340105262010409
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.26993865030674846,
      "acc_stderr": 0.034878251684978906,
      "acc_norm": 0.26993865030674846,
      "acc_norm_stderr": 0.034878251684978906
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.25027932960893856,
      "acc_stderr": 0.014487500852850409,
      "acc_norm": 0.25027932960893856,
      "acc_norm_stderr": 0.014487500852850409
    },
    "hendrycksTest-management": {
      "acc": 0.20388349514563106,
      "acc_stderr": 0.039891398595317706,
      "acc_norm": 0.20388349514563106,
      "acc_norm_stderr": 0.039891398595317706
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.24870466321243523,
      "acc_stderr": 0.0311958408777003,
      "acc_norm": 0.24870466321243523,
      "acc_norm_stderr": 0.0311958408777003
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.22695035460992907,
      "acc_stderr": 0.02498710636564297,
      "acc_norm": 0.22695035460992907,
      "acc_norm_stderr": 0.02498710636564297
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816505,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816505
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.26309067688378035,
      "acc_stderr": 0.015745497169049053,
      "acc_norm": 0.26309067688378035,
      "acc_norm_stderr": 0.015745497169049053
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.20588235294117646,
      "acc_stderr": 0.026265024608275882,
      "acc_norm": 0.20588235294117646,
      "acc_norm_stderr": 0.026265024608275882
    },
    "arc_challenge": {
      "acc": 0.22525597269624573,
      "acc_stderr": 0.0122078399954073,
      "acc_norm": 0.2636518771331058,
      "acc_norm_stderr": 0.012875929151297068
    },
    "hendrycksTest-abstract_algebra": {
      "acc": 0.24,
      "acc_stderr": 0.042923469599092816,
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.042923469599092816
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.25757575757575757,
      "acc_stderr": 0.031156269519646847,
      "acc_norm": 0.25757575757575757,
      "acc_norm_stderr": 0.031156269519646847
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.25738396624472576,
      "acc_stderr": 0.0284588209914603,
      "acc_norm": 0.25738396624472576,
      "acc_norm_stderr": 0.0284588209914603
    },
    "hendrycksTest-astronomy": {
      "acc": 0.27631578947368424,
      "acc_stderr": 0.03639057569952924,
      "acc_norm": 0.27631578947368424,
      "acc_norm_stderr": 0.03639057569952924
    }
  },
  "versions": {
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hellaswag": 0,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-high_school_european_history": 1,
    "truthfulqa_mc": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-world_religions": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "arc_challenge": 0,
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-astronomy": 1
  },
  "config": {
    "model": "stablelm-base-alpha-3b",
    "num_fewshot": 0,
    "batch_size": 4,
    "device": "cuda:0",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 2,
    "description_dict": null
  }
}