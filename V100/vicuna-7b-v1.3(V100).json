{
  "results": {
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.3487394957983193,
      "acc_stderr": 0.03095663632856655,
      "acc_norm": 0.3487394957983193,
      "acc_norm_stderr": 0.03095663632856655
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421276,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421276
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.3229357798165138,
      "acc_stderr": 0.02004811592341532,
      "acc_norm": 0.3229357798165138,
      "acc_norm_stderr": 0.02004811592341532
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.04006168083848876,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.04006168083848876
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909283,
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.04292346959909283
    },
    "hendrycksTest-sociology": {
      "acc": 0.3283582089552239,
      "acc_stderr": 0.03320685889744324,
      "acc_norm": 0.3283582089552239,
      "acc_norm_stderr": 0.03320685889744324
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.35784313725490197,
      "acc_stderr": 0.033644872860882996,
      "acc_norm": 0.35784313725490197,
      "acc_norm_stderr": 0.033644872860882996
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.2630057803468208,
      "acc_stderr": 0.02370309952525816,
      "acc_norm": 0.2630057803468208,
      "acc_norm_stderr": 0.02370309952525816
    },
    "hendrycksTest-prehistory": {
      "acc": 0.2839506172839506,
      "acc_stderr": 0.025089478523765127,
      "acc_norm": 0.2839506172839506,
      "acc_norm_stderr": 0.025089478523765127
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.37823834196891193,
      "acc_stderr": 0.03499807276193338,
      "acc_norm": 0.37823834196891193,
      "acc_norm_stderr": 0.03499807276193338
    },
    "hendrycksTest-public_relations": {
      "acc": 0.2545454545454545,
      "acc_stderr": 0.04172343038705383,
      "acc_norm": 0.2545454545454545,
      "acc_norm_stderr": 0.04172343038705383
    },
    "hendrycksTest-econometrics": {
      "acc": 0.22807017543859648,
      "acc_stderr": 0.03947152782669415,
      "acc_norm": 0.22807017543859648,
      "acc_norm_stderr": 0.03947152782669415
    },
    "hendrycksTest-nutrition": {
      "acc": 0.3104575163398693,
      "acc_stderr": 0.026493033225145908,
      "acc_norm": 0.3104575163398693,
      "acc_norm_stderr": 0.026493033225145908
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.33587786259541985,
      "acc_stderr": 0.041423137719966634,
      "acc_norm": 0.33587786259541985,
      "acc_norm_stderr": 0.041423137719966634
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.32450331125827814,
      "acc_stderr": 0.03822746937658754,
      "acc_norm": 0.32450331125827814,
      "acc_norm_stderr": 0.03822746937658754
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.2911392405063291,
      "acc_stderr": 0.02957160106575337,
      "acc_norm": 0.2911392405063291,
      "acc_norm_stderr": 0.02957160106575337
    },
    "hendrycksTest-management": {
      "acc": 0.27184466019417475,
      "acc_stderr": 0.044052680241409216,
      "acc_norm": 0.27184466019417475,
      "acc_norm_stderr": 0.044052680241409216
    },
    "hendrycksTest-virology": {
      "acc": 0.19879518072289157,
      "acc_stderr": 0.031069390260789437,
      "acc_norm": 0.19879518072289157,
      "acc_norm_stderr": 0.031069390260789437
    },
    "hendrycksTest-human_aging": {
      "acc": 0.2062780269058296,
      "acc_stderr": 0.027157150479563824,
      "acc_norm": 0.2062780269058296,
      "acc_norm_stderr": 0.027157150479563824
    },
    "hendrycksTest-college_biology": {
      "acc": 0.2708333333333333,
      "acc_stderr": 0.03716177437566017,
      "acc_norm": 0.2708333333333333,
      "acc_norm_stderr": 0.03716177437566017
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.21212121212121213,
      "acc_stderr": 0.031922715695482995,
      "acc_norm": 0.21212121212121213,
      "acc_norm_stderr": 0.031922715695482995
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.30357142857142855,
      "acc_stderr": 0.04364226155841044,
      "acc_norm": 0.30357142857142855,
      "acc_norm_stderr": 0.04364226155841044
    },
    "hendrycksTest-global_facts": {
      "acc": 0.27,
      "acc_stderr": 0.0446196043338474,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.0446196043338474
    },
    "arc_challenge": {
      "acc": 0.23464163822525597,
      "acc_stderr": 0.01238387356076866,
      "acc_norm": 0.2627986348122867,
      "acc_norm_stderr": 0.012862523175351331
    },
    "hendrycksTest-international_law": {
      "acc": 0.19834710743801653,
      "acc_stderr": 0.036401182719909456,
      "acc_norm": 0.19834710743801653,
      "acc_norm_stderr": 0.036401182719909456
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.25153374233128833,
      "acc_stderr": 0.034089978868575295,
      "acc_norm": 0.25153374233128833,
      "acc_norm_stderr": 0.034089978868575295
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.38,
      "acc_stderr": 0.04878317312145632,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.04878317312145632
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.2680851063829787,
      "acc_stderr": 0.028957342788342347,
      "acc_norm": 0.2680851063829787,
      "acc_norm_stderr": 0.028957342788342347
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.296551724137931,
      "acc_stderr": 0.038061426873099935,
      "acc_norm": 0.296551724137931,
      "acc_norm_stderr": 0.038061426873099935
    },
    "hendrycksTest-astronomy": {
      "acc": 0.3223684210526316,
      "acc_stderr": 0.03803510248351585,
      "acc_norm": 0.3223684210526316,
      "acc_norm_stderr": 0.03803510248351585
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.26851851851851855,
      "acc_stderr": 0.04284467968052191,
      "acc_norm": 0.26851851851851855,
      "acc_norm_stderr": 0.04284467968052191
    },
    "truthfulqa_mc": {
      "mc1": 0.23378212974296206,
      "mc1_stderr": 0.014816195991931584,
      "mc2": 0.4016453418249051,
      "mc2_stderr": 0.014918213601732292
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.32051282051282054,
      "acc_stderr": 0.02366129639396428,
      "acc_norm": 0.32051282051282054,
      "acc_norm_stderr": 0.02366129639396428
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.40441176470588236,
      "acc_stderr": 0.029812630701569746,
      "acc_norm": 0.40441176470588236,
      "acc_norm_stderr": 0.029812630701569746
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.30140485312899107,
      "acc_stderr": 0.016409091097268794,
      "acc_norm": 0.30140485312899107,
      "acc_norm_stderr": 0.016409091097268794
    },
    "hendrycksTest-anatomy": {
      "acc": 0.2074074074074074,
      "acc_stderr": 0.03502553170678316,
      "acc_norm": 0.2074074074074074,
      "acc_norm_stderr": 0.03502553170678316
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.26296296296296295,
      "acc_stderr": 0.026842057873833713,
      "acc_norm": 0.26296296296296295,
      "acc_norm_stderr": 0.026842057873833713
    },
    "hendrycksTest-security_studies": {
      "acc": 0.39183673469387753,
      "acc_stderr": 0.031251275910891656,
      "acc_norm": 0.39183673469387753,
      "acc_norm_stderr": 0.031251275910891656
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.29,
      "acc_stderr": 0.04560480215720684,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720684
    },
    "hendrycksTest-computer_security": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816506,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816506
    },
    "hendrycksTest-marketing": {
      "acc": 0.2863247863247863,
      "acc_stderr": 0.029614323690456648,
      "acc_norm": 0.2863247863247863,
      "acc_norm_stderr": 0.029614323690456648
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.26,
      "acc_stderr": 0.044084400227680794,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.044084400227680794
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.28078817733990147,
      "acc_stderr": 0.0316185633535861,
      "acc_norm": 0.28078817733990147,
      "acc_norm_stderr": 0.0316185633535861
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.27547169811320754,
      "acc_stderr": 0.02749566368372405,
      "acc_norm": 0.27547169811320754,
      "acc_norm_stderr": 0.02749566368372405
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3225806451612903,
      "acc_stderr": 0.02659308451657228,
      "acc_norm": 0.3225806451612903,
      "acc_norm_stderr": 0.02659308451657228
    },
    "hendrycksTest-college_physics": {
      "acc": 0.28431372549019607,
      "acc_stderr": 0.04488482852329017,
      "acc_norm": 0.28431372549019607,
      "acc_norm_stderr": 0.04488482852329017
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24692737430167597,
      "acc_stderr": 0.014422292204808845,
      "acc_norm": 0.24692737430167597,
      "acc_norm_stderr": 0.014422292204808845
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.3236994219653179,
      "acc_stderr": 0.0356760379963917,
      "acc_norm": 0.3236994219653179,
      "acc_norm_stderr": 0.0356760379963917
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.22340425531914893,
      "acc_stderr": 0.02484792135806396,
      "acc_norm": 0.22340425531914893,
      "acc_norm_stderr": 0.02484792135806396
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909282,
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.04292346959909282
    },
    "hendrycksTest-philosophy": {
      "acc": 0.2508038585209003,
      "acc_stderr": 0.024619771956697168,
      "acc_norm": 0.2508038585209003,
      "acc_norm_stderr": 0.024619771956697168
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.016819028375736393,
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.016819028375736393
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.03358618145732523,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.03358618145732523
    },
    "hendrycksTest-abstract_algebra": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421276,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421276
    },
    "hendrycksTest-professional_law": {
      "acc": 0.25749674054758803,
      "acc_stderr": 0.011167706014904147,
      "acc_norm": 0.25749674054758803,
      "acc_norm_stderr": 0.011167706014904147
    },
    "hendrycksTest-world_religions": {
      "acc": 0.27485380116959063,
      "acc_stderr": 0.03424042924691583,
      "acc_norm": 0.27485380116959063,
      "acc_norm_stderr": 0.03424042924691583
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3055555555555556,
      "acc_stderr": 0.03141554629402544,
      "acc_norm": 0.3055555555555556,
      "acc_norm_stderr": 0.03141554629402544
    },
    "hellaswag": {
      "acc": 0.37651862178848833,
      "acc_stderr": 0.0048352227940065195,
      "acc_norm": 0.48396733718382795,
      "acc_norm_stderr": 0.004987215542259659
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.24867724867724866,
      "acc_stderr": 0.022261817692400175,
      "acc_norm": 0.24867724867724866,
      "acc_norm_stderr": 0.022261817692400175
    }
  },
  "versions": {
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-global_facts": 1,
    "arc_challenge": 0,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-jurisprudence": 1,
    "truthfulqa_mc": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-world_religions": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hellaswag": 0,
    "hendrycksTest-elementary_mathematics": 1
  },
  "config": {
    "model": "vicuna-7b-v1.3",
    "num_fewshot": 0,
    "batch_size": 4,
    "device": "cuda:0",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 2,
    "description_dict": null
  }
}